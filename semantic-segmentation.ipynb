{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom skimage import io\nfrom glob import glob\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path='cityscapes_data/train'\nval_path='cityscapes_data/val'","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images=[]\ntrain_masks=[]\nval_images=[]\nval_masks=[]\ndef load_images(path):\n    temp_img,temp_mask=[],[]\n    images=glob(os.path.join(path,'*.jpg'))\n    for i in tqdm(images):\n        i=cv2.imread(i)\n        i=cv2.normalize(i,None,0,1,cv2.NORM_MINMAX,cv2.CV_32F)\n        img=i[:,:256]\n        msk=i[:,256:]  \n        temp_img.append(img)\n        temp_mask.append(msk)\n    return temp_img,temp_mask\n\ntrain_images,train_masks=load_images(train_path)\nval_images,val_masks=load_images(val_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lrelu(x,threshold=0.1):\n    return tf.maximum(x,x*threshold)\n\ndef conv_layer(x,n_filters,k_size,stride,padding='SAME'):\n    x=tf.layers.conv2d(x,filters=n_filters,kernel_size=k_size,strides=stride,padding=padding)\n    x=tf.nn.relu(x)\n    return x\n\ndef max_pool(x,pool_size):\n    x=tf.layers.max_pooling2d(x,pool_size=pool_size)\n    return x\n\ndef conv_transpose(x,n_filters,k_size,stride,padding='SAME'):\n    x=tf.layers.conv2d_transpose(x,filters=n_filters,kernel_size=k_size,strides=stride,padding=padding)\n    x=tf.nn.relu(x)\n    return x\n\n\n#Placeholders\nimage=tf.placeholder(tf.float32,[None,256,256,3],name='Input_image')\nmask=tf.placeholder(tf.float32,[None,256,256,3],name='Image_mask')\n\n\n#########################Beta Network\n\n#Branch-0\nlayer_1=conv_layer(image,n_filters=64,k_size=4,stride=1)\nmp_1=tf.layers.max_pooling2d(layer_1,pool_size=2,strides=2)\n\nlayer_2=conv_layer(mp_1,n_filters=128,k_size=4,stride=1)\nmp_2=tf.layers.max_pooling2d(layer_2,pool_size=2,strides=2)\n\nlayer_3=conv_layer(mp_2,n_filters=256,k_size=4,stride=1)\nmp_3=tf.layers.max_pooling2d(layer_3,pool_size=2,strides=2)\n\nlayer_4=conv_layer(mp_3,n_filters=512,k_size=4,stride=1)\nmp_4=tf.layers.max_pooling2d(layer_4,pool_size=2,strides=2)\n\nlayer_5=conv_layer(mp_4,n_filters=1024,k_size=4,stride=1)\nmp_5=tf.layers.max_pooling2d(layer_5,pool_size=2,strides=2)\n\n\n#Branch_1\nlayer_b1=conv_layer(image,n_filters=128,k_size=4,stride=1)\nmp_b1=tf.layers.max_pooling2d(layer_b1,pool_size=2,strides=2)\n\nbeta_1=tf.keras.layers.add([layer_2,mp_b1])\n\nlayer_b2=conv_layer(beta_1,n_filters=256,k_size=4,stride=1)\nmp_b2=tf.layers.max_pooling2d(layer_b2,pool_size=2,strides=2)\n\nbeta_2=tf.keras.layers.add([layer_3,mp_b2])\n\nlayer_b3=conv_layer(beta_2,n_filters=512,k_size=4,stride=1)\nmp_b3=tf.layers.max_pooling2d(layer_b3,pool_size=2,strides=2)\n\nbeta_3=tf.keras.layers.add([mp_b3,layer_4])\n\nlayer_b4=conv_layer(beta_3,n_filters=1024,k_size=4,stride=1)\nmp_b4=tf.layers.max_pooling2d(layer_b4,pool_size=2,strides=2)\n\nbeta_4=tf.keras.layers.add([mp_b4,layer_5])\n\nbeta_0=layer_1\n########################################################\n\n\n\n\n#Downsample\n#64\nx_layer_1=conv_layer(image,n_filters=64,k_size=5,stride=1)\nx_layer_1=conv_layer(x_layer_1,n_filters=64,k_size=4,stride=1)\nx_layer_1=conv_layer(x_layer_1,n_filters=64,k_size=4,stride=2)\nx_batch_1=tf.layers.batch_normalization(x_layer_1)#128x128x64\n\n#128\nx_layer_2=conv_layer(x_batch_1,n_filters=128,k_size=5,stride=1)\nx_layer_2=conv_layer(x_layer_2,n_filters=128,k_size=4,stride=1)\nx_layer_2=conv_layer(x_layer_2,n_filters=128,k_size=4,stride=2)\nx_batch_2=tf.layers.batch_normalization(x_layer_2)#64x64x128\n\n#256\nx_layer_3=conv_layer(x_batch_2,n_filters=256,k_size=5,stride=1)\nx_layer_3=conv_layer(x_layer_3,n_filters=256,k_size=4,stride=1)\nx_layer_3=conv_layer(x_layer_3,n_filters=256,k_size=4,stride=2)\nx_batch_3=tf.layers.batch_normalization(x_layer_3)#32x32x256\n\n#512\nx_layer_4=conv_layer(x_batch_3,n_filters=512,k_size=5,stride=1)\nx_layer_4=conv_layer(x_layer_4,n_filters=512,k_size=4,stride=1)\nx_layer_4=conv_layer(x_layer_4,n_filters=512,k_size=4,stride=2)\nx_batch_4=tf.layers.batch_normalization(x_layer_4)#16x16x512\n\n#1024\nx_layer_5=conv_layer(x_batch_4,n_filters=1024,k_size=4,stride=1)\nx_layer_5=conv_layer(x_layer_5,n_filters=1024,k_size=4,stride=8)\nx_batch_5=tf.layers.batch_normalization(x_layer_5)#8x8x1024\n\n\n#Upsample\n#1024\ny_layer_1=conv_transpose(x_batch_5,n_filters=1024,k_size=4,stride=8)\ny_layer_1=tf.keras.layers.add([y_layer_1,beta_4])\ny_layer_1=conv_layer(y_layer_1,n_filters=1024,k_size=4,stride=1)\ny_batch_1=tf.layers.batch_normalization(y_layer_1)\n\n\n#512\ny_layer_2=conv_transpose(y_batch_1,n_filters=512,k_size=5,stride=2)\ny_layer_2=tf.keras.layers.add([y_layer_2,beta_3])\ny_layer_2=conv_layer(y_layer_2,n_filters=512,k_size=4,stride=1)\ny_layer_2=conv_layer(y_layer_2,n_filters=512,k_size=4,stride=1)\ny_batch_2=tf.layers.batch_normalization(y_layer_2)\n\n#256\ny_layer_3=conv_transpose(y_batch_2,n_filters=256,k_size=5,stride=2)\ny_layer_3=tf.keras.layers.add([y_layer_3,beta_2])\ny_layer_3=conv_layer(y_layer_3,n_filters=256,k_size=4,stride=1)\ny_layer_3=conv_layer(y_layer_3,n_filters=256,k_size=4,stride=1)\ny_batch_3=tf.layers.batch_normalization(y_layer_3)\n\n\n#128\ny_layer_4=conv_transpose(y_batch_3,n_filters=128,k_size=3,stride=2)\ny_layer_4=tf.keras.layers.add([y_layer_4,beta_1])\ny_layer_4=conv_layer(y_layer_4,n_filters=128,k_size=2,stride=1)\ny_layer_4=conv_layer(y_layer_4,n_filters=128,k_size=2,stride=1)\ny_batch_4=tf.layers.batch_normalization(y_layer_4)\n\n#64\ny_layer_5=conv_transpose(y_batch_4,n_filters=64,k_size=2,stride=2)\ny_layer_5=tf.keras.layers.add([y_layer_5,beta_0])\ny_layer_5=conv_layer(y_layer_5,n_filters=64,k_size=1,stride=1)\ny_layer_5=conv_layer(y_layer_5,n_filters=64,k_size=1,stride=1)\ny_batch_5=tf.layers.batch_normalization(y_layer_5)\n\n#Output\nout=tf.layers.conv2d(y_batch_5,activation=None,filters=3,kernel_size=1,strides=1,padding='SAME')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=mask,logits=out))\ntrain_op=tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs=100\nbatch_size=25\ntrain_batches=len(train_images)//batch_size\nval_batches=len(val_images)//batch_size\ntrain_loss,val_loss=[],[]\nsaver=tf.train.Saver()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(num_epochs):\n        print('======================================')\n        print('Epoch: ',(epoch+1))\n        print('In Training..')\n        for batch in tqdm(range(train_batches)):\n            train_img_batch=train_images[batch*batch_size:(batch+1)*batch_size]\n            train_msk_batch=train_masks[batch*batch_size:(batch+1)*batch_size]\n            #reshape images and masks\n            train_img_batch=np.reshape(train_img_batch,(len(train_img_batch),256,256,3))\n            train_msk_batch=np.reshape(train_msk_batch,(len(train_msk_batch),256,256,3))\n            t_loss,_=sess.run([loss,train_op],feed_dict={\n                    image:train_img_batch,mask:train_msk_batch})\n            \n        print('In Validation..')\n        for v_batch in tqdm(range(val_batches)):\n            val_img_batch=val_images[v_batch*batch_size:(v_batch+1)*batch_size]\n            val_msk_batch=val_masks[v_batch*batch_size:(v_batch+1)*batch_size]\n            \n            #Reshape batches\n            val_img_batch=np.reshape(val_img_batch,(len(val_img_batch),256,256,3))\n            val_msk_batch=np.reshape(val_msk_batch,(len(val_msk_batch),256,256,3))\n            v_loss,_=sess.run([loss,train_op],feed_dict={image:val_img_batch,mask:val_msk_batch})\n        train_loss.append(t_loss)\n        val_loss.append(v_loss)\n        \n        print('Train Loss: ',t_loss)\n        print('Val Loss: ',v_loss)\n    saver.save(sess, \"model.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saver=tf.train.Saver()\nimg_=train_images[5:10]\nmsk_=train_masks[5:10]\npreds=[]\nw,h=256,256\ncol,row=2,2\n#ckpt_path='../input/semantic-segmentation-code/model.ckpt'\nmodel_path='model.ckpt'\nwith tf.Session() as sess_1:\n    saver.restore(sess_1,model_path)\n    print('Model Restored!')\n    seg=sess_1.run(out,feed_dict={image:img_})\n    seg=sess_1.run(tf.nn.sigmoid(seg))\n    fig=plt.figure(figsize=(8,8))\n    for i in range(1,row*col+1):\n        fig.add_subplot(row,col,i)\n        plt.imshow(seg[i-1])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}